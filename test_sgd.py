import numpy as np
import matplotlib.pyplot as plt

# экспоненциальная функция потерь
def loss(w, x, y):
    M = np.dot(w, x) * y
    return np.exp(-M)


# производная экспоненциальной функции потерь по вектору w
def df(w, x, y):
    M = np.dot(w, x.T) * y
    return -np.exp(-M)*x.T * y


data_x = [(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)]
data_y = [-1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1]

x_train = np.array([[1, x[0], x[1]] for x in data_x])
y_train = np.array(data_y)

n_train = len(x_train)  # размер обучающей выборки
w = [0.0, 0.0, 0.0]  # начальные весовые коэффициенты
nt = np.array([0.5, 0.01, 0.01])  # шаг обучения для каждого параметра w0, w1, w2
lm = 0.01  # значение параметра лямбда для вычисления скользящего экспоненциального среднего
N = 500  # число итераций алгоритма SGD
batch_size = 10 # размер мини-батча (величина K = 10)

Qe = np.mean([loss(w, x, y) for x, y in zip(x_train, y_train)]) # начальное значение среднего эмпирического риска
np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел
Q_plot = [Qe]
for i in range(N):
    k = np.random.randint(0, n_train-batch_size-1)
    e_k =  np.mean([loss(w,x,y) for x,y in zip(x_train[k:k+batch_size], y_train[k:k+batch_size])])
    grad = sum([df(w,x,y) for x,y in zip(x_train[k:k+batch_size], y_train[k:k+batch_size])])/batch_size
    w -= nt* grad

    Qe = lm*e_k + (1-lm)*Qe
    Q_plot.append(Qe)

Q = np.mean(np.dot(w, x_train.T) * y_train < 0)


# Формирование графика разделяющей линии
line_x = np.linspace(0, max(x_train[:, 1]), num=100)
line_y = -w[1] / w[2] * line_x - w[0] / w[2]

# Формирование точек для классов
x_0 = x_train[y_train == 1]
x_1 = x_train[y_train == -1]

# Построение графиков
fig, axs = plt.subplots(2, 1, figsize=(6, 10)) # создание двух подграфиков

# График разделяющей линии
axs[0].scatter(x_0[:, 1], x_0[:, 2], color='red', label='Класс 1')
axs[0].scatter(x_1[:, 1], x_1[:, 2], color='blue', label='Класс -1')
axs[0].plot(line_x, line_y, color='green', label='Разделяющая линия')
axs[0].axis('auto')
axs[0].set_ylabel("x2")
axs[0].set_xlabel("x1")
axs[0].grid(True)
axs[0].legend()
axs[0].set_title("График разделяющей линии")

# График изменения Qe
axs[1].plot(Q_plot)
axs[1].set_xlabel("Итерация")
axs[1].set_ylabel("Средний эмпирический риск Qe")
axs[1].set_title("Изменение среднего эмпирического риска")
axs[1].grid(True)

plt.tight_layout(pad=3.0) # настройка отступов между подграфиками
plt.show()